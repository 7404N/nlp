%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Recursive Neural Network Part-of-Speech Tagger}

\author{Hao Liu \\
  NYU \\
  {\tt email@domain} \\\And
  Jiali Huang \\
  NYU \\
  {\tt email@domain} \\\And
  Robert Dionne \\
  NYU \\
  {\tt robertsdionne@nyu.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  TODO: Write the abstract
\end{abstract}

\section{Introduction}

For our final project, we implemented a recursive neural network part-of-speech tagger based on similar prior work by Socher, Manning and Ng (2010) for building sentence parse trees and work by Collobert and Weston (2011) for their distributed word representations. We wanted to explore natural language processing using neural networks, so we chose a small problem (part-of-speech tagging) which we already had experience with during the third homework assignment and applied these modern techniques in neural network research to the problem.

\subsection{Recursive Neural Networks}

Recursive neural networks are neural network models that allow for cyclic connections between neurons. Generally, recursive networks can have very complex structures, but for our application to part-of-speech tagging, we consider a restricted structure in which the network observes its own output on prior sentence context while consuming as input the current word in order to produce a classification label representing the current word's part of speech tag. When training the network or using the network to classify an example sentence, we temporarily unroll the recurrent network into an equivalent, feedforward multilayer perceptron with the weight matrix shared between all layers.

Our network structure is a simplification of the more general binary tree structure trained by Socher et al. (2010) which iteratively constructs a parse tree by evaluating, at the base case, the best combinations of pairs of words, and subsequently the best combinations of pairs of previous combinations. Instead, we construct a linear, degenerate tree by always pairing the current word (on the right) to the previous combination (on the left) of prior sentence context and the prior word to obtain a phrase representation. We hope that the resulting phrase representation captures both the possible parts-of-speech for the current word and distinguishing context that allows an accurate choice of the correct part-of-speech.

\subsection{Distributed Word Representations}

Underlying the entire model are distributed word representations, which we use directly from the research of Collobert and Weston (2011), downloadable from their website (\hyperlink{http://ronan.collobert.com/senna/}{ronan.collobert.com/senna}). Distributed word representations are dense vectors embedded within an $n$-dimensional vector space that differ from sparse, ``one-hot" vectors embedded within a $\left\vert\mathcal{V}\right\vert$-dimensional vector space, where $\mathcal{V}$ is the vocabulary of the language domain.

The distributed word representations are induced by training a neural network in an unsupervised manner to distinguish a word within its concrete contexts from randomly chosen replacement words within those same contexts, and then back propagating the errors into an $n\times\left\vert\mathcal{V}\right\vert$-dimensional lookup table with randomly initialized values. The lookup table converts between the original ``one-hot" vector representation, which encodes zero information about the word aside from identity, and the superior distributed representation, which encodes a word's contextual information. Since we believe a word's context encodes information about that word's part-of-speech, we hypothesize that distributed word representations will be vital to our part-of-speech tagging model.

\section{Our Models}

TODO: Write about our models.

\subsection{Basic}

TODO: Describe models.

\subsection{Compositional Vector Grammar}

TODO: Describe CVG.

\subsection{Future}

TODO: Describe future models.

\section{Experiments}

TODO: Describe experiments.

\subsection{Backpropagation Through Time}

TODO: Describe back propagation through time.

\subsection{Training Tricks}

TODO: Describe training tricks.

\subsubsection{Weight Initialization}

TODO: Describe weight initialization.

\subsubsection{AdaGrad}

TODO: Describe AdaGrad.

\subsection{Analysis}

TODO: Describe analysis.

\subsubsection{Training Progress Diagram}

TODO: Describe training progress diagrams.

\subsubsection{Comparison to Other Models}

TODO: Compare to baseline.

TODO: Compare to HMM.

\subsubsection{Error Analysis}

TODO: Confusion matrices.

TODO: Histogram for missed tags per sentence.

TODO: Histogram for the related position of missed tags in sentence.

TODO: Compare to maximum likelihood model.

TODO: Specific error examples and hypotheses.

TODO: Compare with and without updating word representations.

TODO: Compare larger hidden layer sizes.


\section{Conclusion}

\section{Academic Honesty Pledge}

Honor Pledge

We pledge our honor that all the work described in this report is solely ours and
that we have given credit to all third party resources that we have used.

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2014}

\begin{thebibliography}{}

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
{American Psychological Association}.
\newblock 1983.
\newblock {\em Publications Manual}.
\newblock American Psychological Association, Washington, DC.

\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
{Association for Computing Machinery}.
\newblock 1983.
\newblock {\em Computing Reviews}, 24(11):503--512.

\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
\newblock 1981.
\newblock Alternation.
\newblock {\em Journal of the Association for Computing Machinery},
  28(1):114--133.

\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
Dan Gusfield.
\newblock 1997.
\newblock {\em Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\end{thebibliography}

\end{document}
