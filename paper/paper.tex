%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Recursive Neural Network Part-of-Speech Tagger}

\author{Hao Liu \\
  NYU \\
  {\tt email@domain} \\\And
  Jiali Huang \\
  NYU \\
  {\tt email@domain} \\\And
  Robert Dionne \\
  NYU \\
  {\tt robertsdionne@nyu.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  TODO: Write the abstract
\end{abstract}

\section{Introduction}

For our final project, we implemented a recursive neural network part-of-speech tagger based on similar prior work by Socher, Manning and Ng (2010) for building sentence parse trees and work by Collobert and Weston (2011) for their distributed word representations. We wanted to explore natural language processing using neural networks, so we chose a small problem (part-of-speech tagging) which we already had experience with during the third homework assignment and applied these modern techniques in neural network research to the problem.

\subsection{Recursive Neural Networks}

Recursive neural networks are neural network models that allow for cyclic connections between neurons. Generally, recursive networks can have very complex structures, but for our application to part-of-speech tagging, we consider a restricted structure in which the network observes its own output on prior sentence context while consuming as input the current word in order to produce a classification label representing the current word's part of speech tag. When training the network or using the network to classify an example sentence, we temporarily unroll the recurrent network into an equivalent, feedforward multilayer perceptron with the weight matrix shared between all layers.

Our network structure is a simplification of the more general binary tree structure trained by Socher et al. (2010) which iteratively constructs a parse tree by evaluating, at the base case, the best combinations of pairs of words, and subsequently the best combinations of pairs of previous combinations. Instead, we construct a linear, degenerate tree by always pairing the current word (on the right) to the previous combination (on the left) of prior sentence context and the prior word to obtain a phrase representation. We hope that the resulting phrase representation captures both the possible parts-of-speech for the current word and distinguishing context that allows an accurate choice of the correct part-of-speech.

\subsection{Distributed Word Representations}

Underlying the entire model are distributed word representations, which we use directly from the research of Collobert and Weston (2011), downloadable from their website (\hyperlink{http://ronan.collobert.com/senna/}{ronan.collobert.com/senna}). Distributed word representations are dense vectors embedded within an $n$-dimensional vector space that differ from sparse, ``one-hot" vectors embedded within a $\left\vert\mathcal{V}\right\vert$-dimensional vector space, where $\mathcal{V}$ is the vocabulary of the language domain.

The distributed word representations are induced by training a neural network in an unsupervised manner to distinguish a word within its concrete contexts from randomly chosen replacement words within those same contexts, and then back propagating the errors into an $n\times\left\vert\mathcal{V}\right\vert$-dimensional lookup table with randomly initialized values. The lookup table converts between the original ``one-hot" vector representation, which encodes zero information about the word aside from identity, and the superior distributed representation, which encodes a word's contextual information. Since we believe a word's context encodes information about that word's part-of-speech, we hypothesize that distributed word representations will be vital to our part-of-speech tagging model.

\section{Our Models}

TODO: Write about our models.

\subsection{Basic}

TODO: Describe models.

\subsection{Compositional Vector Grammar}

TODO: Describe CVG.

\subsection{Future}

TODO: Describe future models.

\section{Experiments}

TODO: Describe experiments.

\subsection{Backpropagation Through Time}

TODO: Describe back propagation through time.

\subsection{Training Tricks}

TODO: Describe training tricks.

\subsubsection{Weight Initialization}

TODO: Describe weight initialization.

\subsubsection{AdaGrad}

TODO: Describe AdaGrad.

\subsection{Analysis}

TODO: Describe analysis.

\subsubsection{Training Progress Diagram}

TODO: Describe training progress diagrams.

\subsubsection{Comparison to Other Models}

TODO: Compare to baseline.

TODO: Compare to HMM.

\subsubsection{Error Analysis}

TODO: Confusion matrices.

TODO: Histogram for missed tags per sentence.

TODO: Histogram for the related position of missed tags in sentence.

TODO: Compare to maximum likelihood model.

TODO: Specific error examples and hypotheses.

TODO: Compare with and without updating word representations.

TODO: Compare larger hidden layer sizes.


\section{Conclusion}

\section{Academic Honesty Pledge}

Honor Pledge

We pledge our honor that all the work described in this report is solely ours and
that we have given credit to all third party resources that we have used.

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2014}

\begin{thebibliography}{}

\bibitem[\protect\citename{Collobert and Weston}2011]{Collobert:11}
Ronan~Collobert, Jason~Weston, Léon~Bottou, Michael~Karlen, Koray~Kavukcuoglu and Pavel~Kuksa.
\newblock 2011.
\newblock Natural Language Processing (Almost) from Scratch.
\newblock {\em Journal of Machine Learning Research 12},  (2011) 2461-2505.
\newblock The MIT Press, Cambridge, MA.

\bibitem[\protect\citename{Collobert, Kavukcuoglu and Farabet}2011]{Collobert:11}
Ronan~Collobert, Koray~Kavukcuoglu and Clément~Farabet.
\newblock 2011.
\newblock Torch7: A matlab-like environment for machine learning.
\newblock {\em BigLearn, NIPS Workshop}.

\bibitem[\protect\citename{Glorot and Bengio}2010]{Glorot:10}
Xavier~Glorot and Yoshua~Bengio.
\newblock 2010.
\newblock Understanding the difficulty of training deep feedforward neural networks.
\newblock {\em International Conference on Artificial Intelligence and Statistics}.

\bibitem[\protect\citename{Socher, Bauer, Manning and Ng}2013]{Socher:13}
Richard~Socher, John~Bauer, Christopher~D. Manning and Andrew~Ng.
\newblock 2013.
\newblock Parsing with Compositional Vector Grammars.
\newblock {\em In Proceedings of the ACL conference}.

\bibitem[\protect\citename{Socher, Manning and Ng}2010]{Socher:10}
Richard~Socher, Christopher~D. Manning and Andrew~Ng.
\newblock 2010.
\newblock Learning Continuous Phrase Representations and
Syntactic Parsing with Recursive Neural Networks.
\newblock {\em Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop}.

\end{thebibliography}

\end{document}
